{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown",
   "metadata": {},
   "source": [
    "# Time Displacement Model Training\n",
    "\n",
    "This notebook trains an XGBoost model to predict time displacement (timing humanization)\n",
    "using cluster-based quantization. The target variable `time_offset` represents how much\n",
    "earlier or later a note is played relative to its cluster centroid.\n",
    "\n",
    "Key insight: Notes in multi-note clusters (chords) provide reliable training data because\n",
    "the cluster centroid represents the \"intended\" beat position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import xgboost\n",
    "\n",
    "from midi_to_df_conversion import midi_files_to_df\n",
    "from midi_utility import get_midi_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "save_model = False  # Set to True to save the trained model\n",
    "model_cache_path = Path(\"model_cache\")\n",
    "time_displacement_model_path = model_cache_path / \"time_displacement.json\"\n",
    "time_displacement_scaler_path = model_cache_path / \"time_displacement_scaler.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load real MIDI data from the cache and extract time displacement features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MIDI files\n",
    "midi_dir = Path(\"midi_data_repaired_cache\")\n",
    "midi_files = get_midi_filepaths(midi_dir)\n",
    "print(f\"Found {len(midi_files)} MIDI files\")\n",
    "\n",
    "# Convert to dataframe with time displacement features\n",
    "df = midi_files_to_df(midi_files, skip_suspicious=True, include_time_displacement=True)\n",
    "print(f\"Total notes: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the time displacement distribution\n",
    "print(\"Time offset statistics (all notes):\")\n",
    "print(df[\"time_offset\"].describe())\n",
    "\n",
    "print(\"\\nTime offset statistics (multi-note clusters only):\")\n",
    "multi_cluster_df = df[df[\"in_multi_cluster\"] == 1]\n",
    "print(multi_cluster_df[\"time_offset\"].describe())\n",
    "\n",
    "print(f\"\\nNotes in multi-note clusters: {len(multi_cluster_df)} ({100*len(multi_cluster_df)/len(df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time offset distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# All notes\n",
    "ax1 = axes[0]\n",
    "offsets = df[\"time_offset\"].values\n",
    "bins = range(int(offsets.min()) - 1, int(offsets.max()) + 2)\n",
    "ax1.hist(offsets, bins=min(100, len(bins)), edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Time offset (ticks)')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title(f'All notes (n={len(offsets)})\\nstd={offsets.std():.1f}')\n",
    "\n",
    "# Multi-note clusters only\n",
    "ax2 = axes[1]\n",
    "multi_offsets = multi_cluster_df[\"time_offset\"].values\n",
    "bins = range(int(multi_offsets.min()) - 1, int(multi_offsets.max()) + 2)\n",
    "ax2.hist(multi_offsets, bins=min(100, len(bins)), edgecolor='black', alpha=0.7, color='green')\n",
    "ax2.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Time offset (ticks)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title(f'Multi-note clusters (n={len(multi_offsets)})\\nstd={multi_offsets.std():.1f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-split-markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Split by song to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by song name to prevent leakage\n",
    "song_names = df[\"name\"].unique()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(song_names)\n",
    "\n",
    "split_idx = int(len(song_names) * 0.8)\n",
    "train_songs = song_names[:split_idx]\n",
    "test_songs = song_names[split_idx:]\n",
    "\n",
    "train_df = df[df[\"name\"].isin(train_songs)].copy()\n",
    "test_df = df[df[\"name\"].isin(test_songs)].copy()\n",
    "\n",
    "print(f\"Train songs: {len(train_songs)}, Test songs: {len(test_songs)}\")\n",
    "print(f\"Train notes: {len(train_df)}, Test notes: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-multi-cluster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Train only on multi-note clusters for more reliable targets\n",
    "# Uncomment to enable:\n",
    "# train_df = train_df[train_df[\"in_multi_cluster\"] == 1].copy()\n",
    "# test_df = test_df[test_df[\"in_multi_cluster\"] == 1].copy()\n",
    "# print(f\"After filtering: Train={len(train_df)}, Test={len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-prep-markdown",
   "metadata": {},
   "source": [
    "## Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop (not features)\n",
    "drop_cols = [\n",
    "    \"midi_track_index\", \"midi_event_index\", \"name\", \"time\",\n",
    "    \"velocity\",  # Don't use velocity as input (will be predicted separately)\n",
    "    \"time_offset\",  # Target variable\n",
    "]\n",
    "\n",
    "# Keep only columns that exist in the dataframe\n",
    "drop_cols = [c for c in drop_cols if c in train_df.columns]\n",
    "\n",
    "# Identify categorical and continuous columns\n",
    "cat_cols = [\n",
    "    col for col in train_df.columns\n",
    "    if not pd.api.types.is_numeric_dtype(train_df[col]) and col not in drop_cols\n",
    "]\n",
    "cont_cols = [\n",
    "    col for col in train_df.columns\n",
    "    if pd.api.types.is_numeric_dtype(train_df[col]) and col not in drop_cols + [\"time_offset\"]\n",
    "]\n",
    "\n",
    "print(f\"Categorical features: {len(cat_cols)}\")\n",
    "print(cat_cols)\n",
    "print(f\"\\nContinuous features: {len(cont_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale continuous features and target\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[cont_cols + [\"time_offset\"]])\n",
    "\n",
    "train_df[cont_cols + [\"time_offset\"]] = scaler.transform(train_df[cont_cols + [\"time_offset\"]])\n",
    "test_df[cont_cols + [\"time_offset\"]] = scaler.transform(test_df[cont_cols + [\"time_offset\"]])\n",
    "\n",
    "print(\"Scaler fitted. Mean of first 5 features:\", scaler.mean_[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-scaler",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model:\n",
    "    os.makedirs(model_cache_path, exist_ok=True)\n",
    "    with open(time_displacement_scaler_path, \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"Saved scaler to {time_displacement_scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convert-categoricals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns\n",
    "for col in cat_cols:\n",
    "    train_df[col] = train_df[col].astype(\"category\")\n",
    "    test_df[col] = test_df[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-xy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "feature_cols = cat_cols + cont_cols\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[\"time_offset\"]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[\"time_offset\"]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "model = xgboost.XGBRegressor(\n",
    "    booster=\"gbtree\",\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=2000,\n",
    "    gamma=0.5,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=0.5,\n",
    "    n_jobs=8,\n",
    "    enable_categorical=True,\n",
    "    early_stopping_rounds=20,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics (in scaled space)\n",
    "train_rmse = mean_squared_error(y_train, train_preds, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, test_preds, squared=False)\n",
    "train_mae = mean_absolute_error(y_train, train_preds)\n",
    "test_mae = mean_absolute_error(y_test, test_preds)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unscale-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time_offset index in the scaler (it's the last column)\n",
    "time_offset_idx = len(cont_cols)  # time_offset is appended after cont_cols\n",
    "time_offset_std = np.sqrt(scaler.var_[time_offset_idx])\n",
    "time_offset_mean = scaler.mean_[time_offset_idx]\n",
    "\n",
    "# Convert metrics to original scale (ticks)\n",
    "print(f\"\\nIn original scale (MIDI ticks):\")\n",
    "print(f\"Train RMSE: {train_rmse * time_offset_std:.2f} ticks\")\n",
    "print(f\"Test RMSE: {test_rmse * time_offset_std:.2f} ticks\")\n",
    "print(f\"Train MAE: {train_mae * time_offset_std:.2f} ticks\")\n",
    "print(f\"Test MAE: {test_mae * time_offset_std:.2f} ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "results = model.evals_result()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results[\"validation_0\"][\"rmse\"], label=\"train\")\n",
    "plt.plot(results[\"validation_1\"][\"rmse\"], label=\"test\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Training History\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sample for plotting\n",
    "sample_idx = np.random.choice(len(y_test), min(1000, len(y_test)), replace=False)\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(y_test.iloc[sample_idx], test_preds[sample_idx], alpha=0.3)\n",
    "ax1.plot([-3, 3], [-3, 3], 'r--', label='Perfect prediction')\n",
    "ax1.set_xlabel('Actual time_offset (scaled)')\n",
    "ax1.set_ylabel('Predicted time_offset (scaled)')\n",
    "ax1.set_title('Actual vs Predicted (Test Set)')\n",
    "ax1.legend()\n",
    "\n",
    "# Residual distribution\n",
    "ax2 = axes[1]\n",
    "residuals = test_preds - y_test.values\n",
    "ax2.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(x=0, color='red', linestyle='--')\n",
    "ax2.set_xlabel('Residual (predicted - actual)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title(f'Residual Distribution\\nmean={residuals.mean():.3f}, std={residuals.std():.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance_map = dict(zip(feature_cols, model.feature_importances_))\n",
    "sorted_importance = sorted(importance_map.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "for name, importance in sorted_importance[:20]:\n",
    "    print(f\"  {name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "top_n = 20\n",
    "names = [x[0] for x in sorted_importance[:top_n]]\n",
    "values = [x[1] for x in sorted_importance[:top_n]]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(names)), values[::-1])\n",
    "plt.yticks(range(len(names)), names[::-1])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title(f'Top {top_n} Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model:\n",
    "    model.save_model(time_displacement_model_path)\n",
    "    print(f\"Saved model to {time_displacement_model_path}\")\n",
    "else:\n",
    "    print(\"Model not saved. Set save_model=True to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"TIME DISPLACEMENT MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f} (scaled), {test_rmse * time_offset_std:.2f} ticks\")\n",
    "print(f\"Test MAE: {test_mae:.4f} (scaled), {test_mae * time_offset_std:.2f} ticks\")\n",
    "print(f\"\\nTop 5 features: {[x[0] for x in sorted_importance[:5]]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
